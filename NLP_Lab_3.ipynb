{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evlko/CS-388/blob/main/NLP_Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAyzWDzFdnLa"
      },
      "source": [
        "## Text Classification: Spam or Ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh91cKPmdnLb"
      },
      "source": [
        "In this task, using the classic Spambase Dataset (https://archive.ics.uci.edu/ml/datasets/spambase) as an example, we will try to make our own spam filter using the scikit-learn library. The dataset contains a text corpus of 5,574 sms labeled \"spam\" and \"ham\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Bf-AmgdnLb"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnJwvQzbdnLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f714105f-972f-499f-bef1-f6cce1adbd46"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/evlko/CS-388/main/data/spam.csv', encoding='latin-1')\n",
        "df = df.drop(df.columns[[2, 3, 4]], axis=1)\n",
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRqzedIIdnLc"
      },
      "source": [
        "Leave only the columns which makes interest for us - SMS texts and tags:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO59-HEadnLc"
      },
      "source": [
        "df = df[['v1', 'v2']]\n",
        "df = df.rename(columns = {'v1': 'label', 'v2': 'text'})"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU2xwmvIdnLd"
      },
      "source": [
        "Remove duplicate texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iveCG_bXdnLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e50df1f-acdf-48c6-bf63-9936452dce3b"
      },
      "source": [
        "df = df.drop_duplicates('text')\n",
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5169, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuPip3mzdnLd"
      },
      "source": [
        "Replacing labels with binary ones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsKdfy6-dnLd"
      },
      "source": [
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vQJK8LNdnLe"
      },
      "source": [
        "### Task 1: Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8tefVdTdnLe"
      },
      "source": [
        "We need to add a function for preprocessing messages, which does the following with the text:\n",
        "* converts text to lowercase;\n",
        "* removes stop words;\n",
        "* removes punctuation marks;\n",
        "* normalizes text using the Snowball stemmer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhU1BvAHdnLe"
      },
      "source": [
        "from nltk import stem\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = stem.SnowballStemmer('english')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    predictions = [stemmer.stem(word) for word in text.split(\" \") if word not in stopwords] \n",
        "    text = ' '.join(predictions)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM-Lrt6ddnLe"
      },
      "source": [
        "Simple tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSuPqUb2dnLe"
      },
      "source": [
        "assert preprocess(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\") == \"im gonna home soon dont want talk stuff anymor tonight k ive cri enough today\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert preprocess(\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\") == \"go jurong point crazi avail bugi n great world la e buffet cine got amor wat\""
      ],
      "metadata": {
        "id": "ZfakVrwpkw_q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84H8mOmpdnLe"
      },
      "source": [
        "Apply the function to the texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtM4626ednLe"
      },
      "source": [
        "df['text'] = df['text'].apply(preprocess)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRyqtqUtdnLe"
      },
      "source": [
        "### Task 2: split the data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt7Z5NCMdnLe"
      },
      "source": [
        "y = df['label'].values"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1c9ARWIdnLe"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.25, random_state=94)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOV7Ub4ldnLe"
      },
      "source": [
        "### Task 3: Classifier Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enAzNefqdnLe"
      },
      "source": [
        "Let's move on to training the classifier.\n",
        "\n",
        "First, we extract features from texts. We recommend trying different ways and see how it affects the result (for more information about the different ways of presenting texts, see https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n",
        "\n",
        "Then we train the classifier. We use SVM, but you can experiment with different algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtfcmJ7NdnLe"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# get features from texts\n",
        "vectorizer = TfidfVectorizer(decode_error='ignore')\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIQQo8j3dnLe"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# train SVM model\n",
        "\n",
        "model = LinearSVC(random_state = 94, C = 1.3)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGxDEYnjdnLe",
        "outputId": "4769cbe5-a1bf-4b49-c5d4-43752e430527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(classification_report(y_test, predictions, digits=3))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.980     0.996     0.988      1133\n",
            "           1      0.965     0.856     0.907       160\n",
            "\n",
            "    accuracy                          0.978      1293\n",
            "   macro avg      0.972     0.926     0.948      1293\n",
            "weighted avg      0.978     0.978     0.978      1293\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1score, support = precision_recall_fscore_support(y_test, predictions)\n",
        "print(\"Precision (macro avg): %.3f\" %(sum(precision)/len(precision)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnQOBFaGlZg6",
        "outputId": "207f20df-e90d-45fb-c604-1b95a37ddde0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision (macro avg): 0.972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nffLu6UdnLf"
      },
      "source": [
        "Let's make a prediction for a specific text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prWswDzudnLf"
      },
      "source": [
        "txt = \"This is all surface tension What a disappointment.\"\n",
        "txt = preprocess(txt)\n",
        "txt = vectorizer.transform([txt])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brfpnzR7dnLf",
        "outputId": "1a856980-2164-445c-d58a-1ea09eda1888",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.predict(txt)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}